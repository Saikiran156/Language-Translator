{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_translator.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP9Kg+bQdpT5VYwqPrBBpmf",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Saikiran156/Git-pack/blob/master/language_translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBwQ1iaSNrzK"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import numpy as np\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "%matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvMjk3fDTBZK"
      },
      "source": [
        "THIS DATA TEXT FILE CONSISTS OF GERMAN-ENGLISH SENTENCE PAIRS .NOW WE HAVE TO READ THE DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4V-wfOYTbDv"
      },
      "source": [
        "#FUNCTION TO READ THE RAW TEXT FILE\n",
        "def read_text(filename):\n",
        "  #opening the file\n",
        "  file = open(filename, mode='rt', encoding='utf-8')\n",
        "  #read all text\n",
        "  text = file.read()\n",
        "  #closing the file\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twdUkrKfUDyF"
      },
      "source": [
        "IT'S TIME TO SPLIT THE PLAIN TEXT INTO ENGLISH-GERMAN PAIRS SEPERATED BY '\\n' AND THEN SPLIT THESE PAIRS INTO ENGLISH AND GERMAN SENTENCES."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGY-7zXYUca-"
      },
      "source": [
        "#split a text into sentences\n",
        "def to_lines(text):\n",
        "  sents = text.strip().split('\\n')\n",
        "  sents = [i.split('\\t') for i in sents]\n",
        "  return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAcV8GGQUvF1"
      },
      "source": [
        "data = read_text(\"/content/deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcPXX4guVgF1"
      },
      "source": [
        "THE ACTUAL DATASET IS VERY HUGE.THEREFORE WE USE FIRST 50000 SENTENCE PAIRS FOR REDUCING THE TRAINING TIME OF THE MODEL "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdGQIkxHV2M1"
      },
      "source": [
        "deu_eng = deu_eng[:500,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iftWp7hDWKqV"
      },
      "source": [
        "#this is our data\n",
        "deu_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hE3tgRHLWg_G"
      },
      "source": [
        "TEXT TO SEQUENCE CONVERSION \n",
        "TO FEED OUR DATA IN A Seq2Seq MODEL, WE WILL HAVE TO CONVERT BOTH INPUT AND THE OUTPUT SENTENCES INTO INTEGER SEQUENCES OF FIXED LENGTH. BEFOR THAT, LET'S VISUALIZE THE LENGTH OF THE SENTENCES. WE WILL CAPTURE THE LENGTHS OF ALL THE SENTENCES IN TWO SEPERATE LISTS FOR ENGLISH AND GERMAN, RESPECTIVELY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhuugTASXl3I"
      },
      "source": [
        "#empty lists\n",
        "eng_1 = []\n",
        "deu_1 = []\n",
        "\n",
        "#populate the lists with sentence lengths\n",
        "for i in deu_eng[:,0]:\n",
        "  eng_1.append(len(i.split()))\n",
        "for i in deu_eng[:,1]:\n",
        "  deu_1.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqv1yNDhYQLp"
      },
      "source": [
        "length_df = pd.DataFrame({'eng':eng_1, 'deu':deu_1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znufba4QYiiU"
      },
      "source": [
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1aCIlscY8bm"
      },
      "source": [
        "THE MAXIMUM LENGTH OF GERMAN SENTENCES IS 9 AND THAT OF THE ENGLISH PHRASES IS 8.\n",
        "LET'S VECTORIZE OUR TEXT DATA BY USING KERAS'S TOKENIZER() CLASS. IT WILL TURN OUR SENTENCES INTO SEQUENCES OF INTEGERS. THEN WE WILL PAD THOSE SEQUENCES WITH ZEROS TO MAKE ALL THE SENTENCES OF SAME LENGTH."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFXhChPLZ2Cr"
      },
      "source": [
        "#funtion to build a tokenizer\n",
        "def tokenization(lines):\n",
        "  tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(lines)\n",
        "  return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI4-Req9aMUm",
        "outputId": "c37f1ce6-9d8a-4c1a-81a3-ee2e10f3e2a4"
      },
      "source": [
        "#prepare english tokenizer\n",
        "eng_tokenizer = tokenization(deu_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
        "\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English Vocabulary Size: 217\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBxzSTHOa135",
        "outputId": "01e48490-c386-462b-b3d1-eb1a98edadc5"
      },
      "source": [
        "# prepare Deutch tokenizer\n",
        "deu_tokenizer = tokenization(deu_eng[:, 1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "\n",
        "deu_length = 8\n",
        "print('Deutch Vocabulary Size: %d' % deu_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deutch Vocabulary Size: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62l1ePUsbsMD"
      },
      "source": [
        "#HERE IS THE FUNCTION TO PREPARE THE SEQUENCES.IT WILL ALSO PERFORM SEQUENCE PADDING TO A MAXIMUM SENTENCE LENGTH AS MENTIONED ABOVE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS2CqDBycCrV"
      },
      "source": [
        "#encode and pad sequences\n",
        "def encode_sequences(tokenizer,length,lines):\n",
        "  #integer encode sequences\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  #pad sequences with 0 values\n",
        "  seq = pad_sequences(seq,maxlen=length,padding='post')\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-RNZIXYcnrd"
      },
      "source": [
        "MODEL BUILDING\n",
        "WE WILL NOW SPLIT THE DATA INTO TRAIN AND TEST SET FOR MODEL TRAINING AND EVALUATION,RESPECTIVELY"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQElZU7Ec3dw"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train,test = train_test_split(deu_eng,test_size=0.2,random_state = 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATfPAq23dOC_"
      },
      "source": [
        "IT'S TIME TO ENCODE THE SENTENCES. WE WILL ENCODE GERMAN SENTENCES AS THE INPUT SEQUENCES AND ENGLISH SENTENCES AS THE TARGET SEQUENCES.IT WILL BE DONE FOR BOTH TRAIN AND TEST DATASETS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l-aMgn0dktq"
      },
      "source": [
        "# preparing training data\n",
        "trainX = encode_sequences(deu_tokenizer, deu_length, test[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaImMEc1eCMC"
      },
      "source": [
        "# preparing validation data\n",
        "testX = encode_sequences(deu_tokenizer, deu_length,test[:, 1])\n",
        "testY = encode_sequences(eng_tokenizer, eng_length,test[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30GX8yMsekL7"
      },
      "source": [
        "NOW COMES THE EXICITING PART! LET US DEFINE OUR Seq2Seq MODEL ARCHITECTURE.WE ARE USING AN EMBEDDING LAYER AND AN LSTM LAYER AS OUR ENCODER AND ANOTHER LSTM LAYER FOLLOWED BY A DENSE LAYER AS THE DECODER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3cCwFesfAkv"
      },
      "source": [
        "# BUILD NMT MODEL\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "  model.add(LSTM(units))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(LSTM(units, return_sequences=True))\n",
        "  model.add(Dense(out_vocab, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7iyT4BEr8PF"
      },
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "ms = tf.optimizers.RMSprop(learning_rate=0.001)\n",
        "model.compile(optimizer=ms, loss='sparse_categorical_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrXKQhlOgczb"
      },
      "source": [
        "WE ARE USING RMSprop OPTIMIZER IN THIS MODEL AS IT IS USUALLY A GOOD CHOICE FOR RECURRENT NEURAL NETWORKS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf4B-Z_z3QIo"
      },
      "source": [
        "IT'S TIME TO START TRAINING THE MODEL.NOW TRAIN IT FOR 30 EPOCHS AND WITH A BATCH SIZE OF 512.NOW WE WILL ALSO BE USING ModelCheckpoint() to \n",
        "save the best model with lowest validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20AhpRJp3t96"
      },
      "source": [
        "filename = 'model.h1.24_aashutosh'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1),\n",
        "                    epochs=5, batch_size=512,\n",
        "                    validation_split = 0.2,\n",
        "                    callbacks=[checkpoint], verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dRyJQrF5HO2"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RH7tg7549Jgd"
      },
      "source": [
        "model = load_model('model.h1.24_aashutosh')\n",
        "preds = model.predict(testX.reshape((testX.shape[0],testX.shape[1])))\n",
        "preds = np.argwhere(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9268VYk_mjW"
      },
      "source": [
        "def get_word(n, tokenizer):\n",
        "  for word,index in tokenizer.word_index.items():\n",
        "    if index == n:\n",
        "      return word\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvPwahO6ANaQ"
      },
      "source": [
        "#convert predicitons into text (ENGLISH)\n",
        "preds_text = []\n",
        "for i in preds:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], eng_tokenizer)\n",
        "    if j>0:\n",
        "       if(t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "          temp.append('')\n",
        "       else:\n",
        "          temp.append(t)\n",
        "    \n",
        "    else:\n",
        "        if(t == None):\n",
        "            temp.append('')\n",
        "        else:\n",
        "            temp.append(t)\n",
        "\n",
        "  preds_text.append(''.join(temp))\n",
        "  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YHpBp3wBw7N"
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0]})\n",
        "preds_text = array(preds_text)\n",
        "a = pd.DataFrame({'predicted' : preds_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7mBek43CGB1"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "PJhFUe4BP4JX",
        "outputId": "e2ec5373-a08c-4301-c0bb-c39ad226293f"
      },
      "source": [
        "frames = [pred_df,a]\n",
        "result = pd.concat(frames,axis=1)\n",
        "result.head(20)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>actual</th>\n",
              "      <th>predicted</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I'm full.</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Get lost!</td>\n",
              "      <td>i</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Go away!</td>\n",
              "      <td>it</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Ask me.</td>\n",
              "      <td>away</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Find Tom.</td>\n",
              "      <td>i'm</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Ask me.</td>\n",
              "      <td>get</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>It's me.</td>\n",
              "      <td>go</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>I'm deaf.</td>\n",
              "      <td>tom</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Wow!</td>\n",
              "      <td>me</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Go away!</td>\n",
              "      <td>lost</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Wait!</td>\n",
              "      <td>up</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>It works.</td>\n",
              "      <td>beat</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Hi, Tom.</td>\n",
              "      <td>be</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Get away!</td>\n",
              "      <td>he</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Catch me.</td>\n",
              "      <td>on</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Lie low.</td>\n",
              "      <td>help</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Keep it.</td>\n",
              "      <td>got</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Have fun.</td>\n",
              "      <td>hug</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Help us.</td>\n",
              "      <td>keep</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Wake up!</td>\n",
              "      <td>lie</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       actual predicted\n",
              "0   I'm full.          \n",
              "1   Get lost!         i\n",
              "2    Go away!        it\n",
              "3     Ask me.      away\n",
              "4   Find Tom.       i'm\n",
              "5     Ask me.       get\n",
              "6    It's me.        go\n",
              "7   I'm deaf.       tom\n",
              "8        Wow!        me\n",
              "9    Go away!      lost\n",
              "10      Wait!        up\n",
              "11  It works.      beat\n",
              "12   Hi, Tom.        be\n",
              "13  Get away!        he\n",
              "14  Catch me.        on\n",
              "15   Lie low.      help\n",
              "16   Keep it.       got\n",
              "17  Have fun.       hug\n",
              "18   Help us.      keep\n",
              "19   Wake up!       lie"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    }
  ]
}